import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from openai import OpenAI

# 1. KONFIGURACJA STRONY (Musi byÄ‡ na samym poczÄ…tku)
st.set_page_config(page_title="SEO URL Analyzer", page_icon="ğŸ”")

# 2. INICJALIZACJA KLIENTA OPENAI
# To naprawia bÅ‚Ä…d "NameError: name 'client' is not defined"
try:
    api_key = st.secrets["OPENAI_API_KEY"]
    client = OpenAI(api_key=api_key)
except Exception as e:
    st.error("âš ï¸ Brak klucza API w secrets! Upewnij siÄ™, Å¼e dodaÅ‚eÅ› OPENAI_API_KEY w ustawieniach Streamlit.")
    client = None

# --- FUNKCJE POMOCNICZE ---

def get_seo_metadata(url):
    """Pobiera Title i Meta Description z podanego URL."""
    try:
        # Udajemy przeglÄ…darkÄ™, Å¼eby serwery nas nie blokowaÅ‚y
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Pobieranie Title
        title_tag = soup.find('title')
        title = title_tag.get_text().strip() if title_tag else ""
        
        # Pobieranie Meta Description
        # Szukamy zarÃ³wno 'description' jak i 'Description' (wielkoÅ›Ä‡ liter ma znaczenie w kodzie, choÄ‡ nie w HTML)
        meta_desc_tag = soup.find('meta', attrs={'name': 'description'})
        if not meta_desc_tag:
            meta_desc_tag = soup.find('meta', attrs={'name': 'Description'})
            
        description = meta_desc_tag['content'].strip() if meta_desc_tag and 'content' in meta_desc_tag.attrs else ""
        
        return title, description
    except Exception as e:
        # W razie bÅ‚Ä™du zwracamy None, Å¼eby potem oznaczyÄ‡ status jako BÅ‚Ä…d
        return None, None 

def generate_target_keyword(title, description, client):
    """UÅ¼ywa AI do zgadniÄ™cia frazy kluczowej na podstawie meta tagÃ³w."""
    
    # JeÅ›li scraping siÄ™ nie udaÅ‚, nie pytamy AI
    if not title and not description:
        return "BÅ‚Ä…d pobierania danych"
        
    prompt = f"""
    JesteÅ› ekspertem SEO. Przeanalizuj poniÅ¼sze dane ze strony internetowej:
    
    Meta Title: {title}
    Meta Description: {description}
    
    Zadanie: Zidentyfikuj JEDNÄ„ gÅ‚Ã³wnÄ… frazÄ™ kluczowÄ… (Main Keyword), pod ktÃ³rÄ… ta strona jest najprawdopodobniej optymalizowana.
    Wypisz tylko tÄ™ frazÄ™, bez cudzysÅ‚owÃ³w i zbÄ™dnych komentarzy.
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini", # Model mini jest idealny do tego zadania (tani i szybki)
            messages=[
                {"role": "system", "content": "You are a helpful SEO assistant."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"BÅ‚Ä…d AI: {str(e)}"

# --- UI (INTERFEJS UÅ»YTKOWNIKA) ---

st.header("ğŸ” Generator Frazy z URLi")
st.markdown("""
To narzÄ™dzie wchodzi na podane strony, pobiera ich **Meta Title** i **Description**, 
a nastÄ™pnie prosi AI o wskazanie, na jakÄ… **frazÄ™ kluczowÄ…** strona jest pozycjonowana.
""")

# Pole tekstowe na URLe
urls_input = st.text_area(
    "Wklej adresy URL (kaÅ¼dy w nowej linii):",
    height=150,
    placeholder="https://przyklad.pl/podstrona1\nhttps://przyklad.pl/podstrona2"
)

if st.button("ğŸš€ Analizuj URLe i generuj frazy", type="primary"):
    
    # Walidacja klienta OpenAI przed startem
    if not client:
        st.error("Nie moÅ¼na uruchomiÄ‡ analizy bez poprawnego klucza API.")
        st.stop()

    # Przygotowanie listy URLi (usuwanie pustych linii)
    url_list = [url.strip() for url in urls_input.split('\n') if url.strip()]
    
    if not url_list:
        st.warning("Musisz podaÄ‡ przynajmniej jeden adres URL.")
    else:
        results = []
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        total_urls = len(url_list)
        
        # --- GÅÃ“WNA PÄ˜TLA ---
        for i, url in enumerate(url_list):
            # WyÅ›wietlanie aktualnie przetwarzanego linku
            status_text.text(f"â³ Przetwarzanie ({i+1}/{total_urls}): {url}")
            
            # Krok 1: Scraping (Pobieranie danych ze strony)
            title, desc = get_seo_metadata(url)
            
            # Krok 2: AI (Analiza danych)
            if title is not None:
                suggested_keyword = generate_target_keyword(title, desc, client)
                status = "Sukces"
            else:
                title = "BÅ‚Ä…d pobierania"
                desc = "-"
                suggested_keyword = "-"
                status = "BÅ‚Ä…d HTTP/404"
            
            # Zapisanie wyniku do listy
            results.append({
                "URL": url,
                "Meta Title": title,
                "Meta Description": desc,
                "AI Proponowana Fraza": suggested_keyword,
                "Status": status
            })
            
            # Aktualizacja paska postÄ™pu
            progress_bar.progress((i + 1) / total_urls)
            
        # --- KONIEC PÄ˜TLI ---
        progress_bar.empty()
        status_text.success("âœ… ZakoÅ„czono analizÄ™ wszystkich linkÃ³w!")
        
        # WyÅ›wietlenie wynikÃ³w w tabeli
        df_results = pd.DataFrame(results)
        st.dataframe(df_results, use_container_width=True)
        
        # Przycisk pobierania CSV
        csv_data = df_results.to_csv(sep=';', index=False).encode('utf-8')
        st.download_button(
            label="ğŸ“¥ Pobierz wyniki (CSV Excel)",
            data=csv_data,
            file_name="analiza_seo_urls.csv",
            mime="text/csv"
        )
