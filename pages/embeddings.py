import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
import io
import logging
import json
import bcrypt
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# --- KONFIGURACJA LOGOWANIA ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# 1. KONFIGURACJA STRONY (Musi byƒá na samym poczƒÖtku)
st.set_page_config(
    page_title="SEO Embeddingi i Cosinusy", 
    page_icon="üß†",
    layout="wide"
)

# ==========================================
# KONFIGURACJA UWIERZYTELNIANIA (BCRYPT)
# ==========================================
USER_DATA_PATH = 'users.json'  # ≈öcie≈ºka do pliku z u≈ºytkownikami

def check_password(hashed_password, user_password):
    return bcrypt.checkpw(user_password.encode('utf-8'), hashed_password.encode('utf-8'))

def load_users():
    try:
        with open(USER_DATA_PATH, 'r') as file:
            users = json.load(file)
        return users['users']
    except FileNotFoundError:
        st.error(f"Nie znaleziono pliku {USER_DATA_PATH}. Upewnij siƒô, ≈ºe plik istnieje.")
        return {}
    except Exception as e:
        st.error(f"B≈ÇƒÖd odczytu pliku u≈ºytkownik√≥w: {e}")
        return {}

def login(users):
    st.title("üîê Logowanie do SEO Maceratora")
    username = st.text_input("Nazwa u≈ºytkownika")
    password = st.text_input("Has≈Ço", type="password")
    
    if st.button("Zaloguj"):
        if username in users and check_password(users[username], password):
            st.session_state['logged_in'] = True
            st.session_state['username'] = username
            st.success("Zalogowano pomy≈õlnie!")
            st.rerun()
        else:
            st.error("Nieprawid≈Çowa nazwa u≈ºytkownika lub has≈Ço")

def logout():
    st.session_state['logged_in'] = False
    st.session_state['username'] = None
    st.success("Wylogowano pomy≈õlnie!")
    st.rerun()

# ==========================================
# LOGIKA LOGOWANIA (G≈Å√ìWNY PRZEP≈ÅYW)
# ==========================================

# Inicjalizacja stanu sesji
if 'logged_in' not in st.session_state:
    st.session_state['logged_in'] = False
if 'username' not in st.session_state:
    st.session_state['username'] = None

# ≈Åadowanie u≈ºytkownik√≥w
users = load_users()

# Je≈õli nie zalogowany -> Poka≈º ekran logowania i zatrzymaj resztƒô
if not st.session_state['logged_in']:
    login(users)
    st.stop()

# --- PASEK BOCZNY (SIDEBAR) ---
st.sidebar.title(f"üë§ {st.session_state['username']}")
if st.sidebar.button("Wyloguj"):
    logout()

# 2. INICJALIZACJA KLIENTA OPENAI
try:
    api_key = st.secrets["OPENAI_API_KEY"]
    client = OpenAI(api_key=api_key)
except Exception:
    client = None

# --- FUNKCJE POMOCNICZE ---
def get_semantic_template_v2():
    """Generuje wz√≥r pliku dla narzƒôdzia semantycznego"""
    return pd.DataFrame({
        'Keyword': ['buty do biegania', 'krem nawil≈ºajƒÖcy'],
        'Input1 (np. Title)': ['Najlepsze obuwie sportowe Nike', 'Krem do twarzy na dzie≈Ñ'],
        'Input2 (np. Desc)': ['Sprawd≈∫ naszƒÖ ofertƒô but√≥w do biegania w terenie.', 'Lekka formu≈Ça nawil≈ºajƒÖca sk√≥rƒô.']
    })

def get_embedding(text, client):
    """Pobiera wektor z OpenAI (text-embedding-3-large)."""
    # Zabezpieczenie przed pustymi polami (NaN) lub brakiem tekstu
    if not isinstance(text, str) or not text.strip():
        return np.zeros(3072) # Zwraca wektor zerowy

    text = text.replace("\n", " ")
    try:
        return client.embeddings.create(
            input=[text],
            model="text-embedding-3-large"
        ).data[0].embedding
    except Exception as e:
        # W razie b≈Çƒôdu zwracamy wektor zerowy, ≈ºeby nie wywaliƒá ca≈Çego procesu
        return np.zeros(3072)

def cosine_similarity(a, b):
    """Oblicza podobie≈Ñstwo (0 do 1)."""
    if np.all(a == 0) or np.all(b == 0):
        return 0.0
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def extract_clean_text(url):
    """Scraper wycinajƒÖcy menu i stopki (Anti-Boilerplate)."""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200: return None
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Usuwamy techniczne
        for element in soup(["script", "style", "nav", "footer", "header", "aside", "form", "iframe", "noscript", "svg"]):
            element.decompose()

        text = soup.get_text(separator='\n')
        lines = text.split('\n')
        clean_lines = []
        
        for line in lines:
            line = line.strip()
            if len(line) < 30: continue
            garbage_words = ["wszelkie prawa", "copyright", "polityka prywatno≈õci", "pliki cookies", "zobacz wiƒôcej", "czytaj dalej"]
            if any(word in line.lower() for word in garbage_words): continue
            clean_lines.append(line)
        
        final_text = ' '.join(clean_lines)
        if len(final_text) < 100: return soup.get_text(separator=' ')[:5000]
        return final_text[:20000]
    except Exception:
        return None


def get_template_csv():
    """Generuje przyk≈Çadowy plik CSV do pobrania."""
    data = [
        {
            "url": "https://sklep.pl/buty-biegowe", 
            "meta title": "Najlepsze Buty do Biegania - Sklep X", 
            "meta description": "Sprawd≈∫ naszƒÖ ofertƒô but√≥w..."
        },
        {
            "url": "https://sklep.pl/blog/jak-biegac", 
            "meta title": "Jak zaczƒÖƒá biegaƒá? Poradnik", 
            "meta description": "5 porad dla poczƒÖtkujƒÖcych..."
        }
    ]
    df = pd.DataFrame(data)
    return df.to_csv(sep=';', index=False).encode('utf-8')

def get_seo_metadata(url):
    """Pobiera Title i Meta Description ze strony www (Scraping)."""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        title_tag = soup.find('title')
        title = title_tag.get_text().strip() if title_tag else ""
        
        meta_desc_tag = soup.find('meta', attrs={'name': 'description'})
        if not meta_desc_tag:
            meta_desc_tag = soup.find('meta', attrs={'name': 'Description'})
            
        description = meta_desc_tag['content'].strip() if meta_desc_tag and 'content' in meta_desc_tag.attrs else ""
        
        return title, description
    except Exception:
        return None, None

def generate_keyword_ai(url, title, description, user_instructions, client):
    """
    G≈Ç√≥wna funkcja z Twoim nowym PROMPTEM.
    """
    # Zabezpieczenie danych
    if pd.isna(title): title = ""
    if pd.isna(description): description = ""
    if pd.isna(url): url = ""

    if not title and not description:
        return "Brak danych"

    # --- TW√ìJ NOWY PROMPT ---
    prompt = f"""
    Jeste≈õ Ekspertem SEO i SpecjalistƒÖ ds. Semantyki. Twoim zadaniem jest przeanalizowanie danych wej≈õciowych i wyekstrahowanie JEDNEJ, najbardziej trafnej g≈Ç√≥wnej frazy kluczowej (Main Keyword).

    ### DANE WEJ≈öCIOWE:
    URL: {url}
    Title: {title}
    Description: {description}

    ### DODATKOWE INSTRUKCJE OD U≈ªYTKOWNIKA:
    "{user_instructions}"

    ### ZASADY ANALIZY (PRIORYTETY):
    1. Okre≈õl typ strony na podstawie URL i Title.
    2. Wybierz frazƒô zgodnie z poni≈ºszƒÖ logikƒÖ:
       - PRODUKT: Skup siƒô na [Nazwa Producenta] + [Model] + [Rodzaj produktu] (np. "Nike Air Max buty do biegania").
       - KATEGORIA: Skup siƒô na og√≥lnej nazwie asortymentu (np. "Laptopy gamingowe").
       - BLOG/ARTYKU≈Å: Skup siƒô na problemie lub pytaniu, kt√≥re rozwiƒÖzuje tekst (np. "Jak wyczy≈õciƒá buty zamszowe").
       - HOME: Skup siƒô na nazwie Brandu lub g≈Ç√≥wnej us≈Çudze (np. "Agencja SEO Warszawa").
    3. **Hierarchia wa≈ºno≈õci:** Najwa≈ºniejsze s≈Çowa kluczowe znajdujƒÖ siƒô zazwyczaj w `Title`, nastƒôpnie w `URL`, a na ko≈Ñcu w `Description`.

    ### PRZYK≈ÅADY (FEW-SHOT):
    Input: URL: /buty/meskie/nike-air, Title: Nike Air Max - Sklep Online, Desc: Najlepsze buty sportowe...
    Output: Buty mƒôskie Nike Air Max

    Input: URL: /blog/jak-wiazac-krawat, Title: Poradnik eleganta - wiƒÖzanie krawata, Desc: Zobacz 5 sposob√≥w...
    Output: jak wiƒÖzaƒá krawat

    Input: URL: /kontakt, Title: Skontaktuj siƒô z nami - Firma X, Desc: Adres i telefon...
    Output: Firma X kontakt

    ### FORMAT WYJ≈öCIOWY:
    - Zwr√≥ƒá WY≈ÅƒÑCZNIE samƒÖ frazƒô kluczowƒÖ.
    - Nie u≈ºywaj cudzys≈Çow√≥w, punktor√≥w ani znak√≥w interpunkcyjnych na ko≈Ñcu.
    - Nie pisz "Oto fraza" ani ≈ºadnych wyja≈õnie≈Ñ.
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a helpful SEO assistant."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.0
        )
        return response.choices[0].message.content.strip()
    except Exception:
        return "B≈ÇƒÖd API"

# --- INTERFEJS U≈ªYTKOWNIKA ---

st.title("üß† SEO Embeddingi i Cosinusy")
st.markdown("Narzƒôdzie generuje pliki wsadowe (CSV) gotowe do analizy embeddingowej.")

tab1, tab2, tab3, tab4 = st.tabs([
    "üåç 1. Generowanie keyword (Z URLi)", 
    "üìÇ 2. Generowanie keyword (Z CSV)",
    "üß† 3. Analiza Embeddingowa",
    "üï∏Ô∏è 4. Site Focus & Site Radius"
])
# ==========================================
# ZAK≈ÅADKA 1: SCRAPING
# ==========================================
with tab1:
    st.subheader("Generuj z listy URLi")
    
    c1, c2 = st.columns([2, 1])
    with c1:
        urls_input = st.text_area(
            "Lista adres√≥w URL (jeden pod drugim):",
            height=250,
            placeholder="https://sklep.pl/kategoria\nhttps://sklep.pl/produkt-abc"
        )
    with c2:
        st.info("üí° Jak to dzia≈Ça?")
        st.markdown("System wejdzie na ka≈ºdƒÖ stronƒô, pobierze Meta Title i Description, a potem AI wyznaczy frazƒô.")
        # Pole na instrukcje u≈ºytkownika (by≈Ço wcze≈õniej, zostaje)
        user_prefs_t1 = st.text_area(
            "Twoje instrukcje dla AI:",
            height=130,
            placeholder="Np. 'Dla produkt√≥w dodawaj s≈Çowo cena'.",
            key="prefs_tab1"
        )

    if st.button("üöÄ Uruchom Scraping i Analizƒô", key="btn_tab1"):
        if not client:
            st.error("Brak klucza API w secrets!")
            st.stop()
            
        url_list = [u.strip() for u in urls_input.split('\n') if u.strip()]
        
        if not url_list:
            st.warning("Podaj listƒô URLi.")
        else:
            results_t1 = []
            progress_bar = st.progress(0)
            status_text = st.empty()
            total = len(url_list)
            
            for i, url in enumerate(url_list):
                status_text.text(f"Pobieranie ({i+1}/{total}): {url}")
                
                title, desc = get_seo_metadata(url)
                
                if title is not None:
                    # U≈ºywamy instrukcji z Tab 1
                    keyword = generate_keyword_ai(url, title, desc, user_prefs_t1, client)
                else:
                    title, desc, keyword = "B≈ÇƒÖd", "B≈ÇƒÖd", "B≈ÇƒÖd"
                
                results_t1.append({
                    "fraza": keyword,
                    "meta title": title,
                    "meta description": desc,
                    "url": url
                })
                progress_bar.progress((i + 1) / total)
            
            progress_bar.empty()
            status_text.success("‚úÖ Gotowe!")
            
            df_t1 = pd.DataFrame(results_t1)
            df_t1 = df_t1[["fraza", "meta title", "meta description", "url"]]
            
            st.dataframe(df_t1, use_container_width=True)
            st.download_button(
                "üì• Pobierz CSV (Wynik)",
                df_t1.to_csv(sep=';', index=False).encode('utf-8'),
                "wynik_scraping.csv",
                "text/csv"
            )

# ==========================================
# ZAK≈ÅADKA 2: Z PLIKU CSV
# ==========================================
with tab2:
    st.subheader("Generuj z gotowych danych")


    # --- DODANA SEKCJA Z INSTRUKCJƒÑ DLA U≈ªYTKOWNIKA ---
    st.info("""
    **‚ÑπÔ∏è Instrukcja przygotowania pliku:**
    1. Plik musi byƒá formatu **CSV**.
    2. Separator kolumn to **≈õrednik (;)**.
    3. Plik powinien zawieraƒá kolumny z danymi: **URL, Title, Description** (nazwy kolumn mogƒÖ byƒá inne, dopasujesz je poni≈ºej).
    """)
    # --------------------------------------------------
    
    col_d1, col_d2 = st.columns([1, 3])
    with col_d1:
        # NOWO≈öƒÜ: Przycisk pobierania wzoru
        st.download_button(
            label="üìÑ Pobierz wz√≥r pliku CSV",
            data=get_template_csv(),
            file_name="wzor_danych.csv",
            mime="text/csv",
            help="Pobierz przyk≈Çadowy plik, aby zobaczyƒá wymaganƒÖ strukturƒô."
        )
    with col_d2:
        st.markdown("<- Pobierz wz√≥r, je≈õli nie wiesz jak przygotowaƒá plik.")

    st.divider()

    uploaded_file = st.file_uploader("Wgraj sw√≥j plik CSV (separator ≈õrednik ';')", type=['csv'])
    
    if uploaded_file:
        try:
            df_in = pd.read_csv(uploaded_file, sep=';', on_bad_lines='skip')
            st.success(f"Wczytano {len(df_in)} wierszy.")
            
            st.markdown("#### 1. Mapowanie kolumn")
            cols = df_in.columns.tolist()
            col_c1, col_c2, col_c3 = st.columns(3)
            
            # Auto-wykrywanie kolumn
            idx_url = next((i for i, c in enumerate(cols) if 'url' in c.lower()), 0)
            idx_tit = next((i for i, c in enumerate(cols) if 'title' in c.lower()), 0)
            idx_des = next((i for i, c in enumerate(cols) if 'desc' in c.lower()), 0)
            
            with col_c1: sel_url = st.selectbox("Kolumna URL:", cols, index=idx_url)
            with col_c2: sel_tit = st.selectbox("Kolumna Title:", cols, index=idx_tit)
            with col_c3: sel_des = st.selectbox("Kolumna Desc:", cols, index=idx_des)
            
            # NOWO≈öƒÜ: Dodane pole na instrukcje u≈ºytkownika w zak≈Çadce 2 (zamienione na text_area)
            st.markdown("#### 2. Dodatkowe instrukcje")
            user_prefs_t2 = st.text_area(
                "Twoje instrukcje dla AI (opcjonalne):", 
                height=130,
                placeholder="Np. Ignoruj nazwy marek w kategoriach.", 
                key="prefs_tab2"
            )
            
            if st.button("üöÄ Generuj Frazy z pliku", key="btn_tab2"):
                if not client:
                    st.error("Brak klucza API!")
                    st.stop()
                
                results_t2 = []
                prog_bar_t2 = st.progress(0)
                total_rows = len(df_in)
                
                for i, row in df_in.iterrows():
                    u_val = str(row[sel_url])
                    t_val = str(row[sel_tit])
                    d_val = str(row[sel_des])
                    
                    # U≈ºywamy instrukcji z Tab 2
                    kw = generate_keyword_ai(u_val, t_val, d_val, user_prefs_t2, client)
                    
                    results_t2.append({
                        "fraza": kw,
                        "meta title": t_val,
                        "meta description": d_val,
                        "url": u_val
                    })
                    
                    if i % 5 == 0 or i == total_rows - 1:
                        prog_bar_t2.progress((i + 1) / total_rows)
                
                prog_bar_t2.empty()
                st.success("‚úÖ Zako≈Ñczono!")
                
                df_t2 = pd.DataFrame(results_t2)
                df_t2 = df_t2[["fraza", "meta title", "meta description", "url"]]
                
                st.dataframe(df_t2, use_container_width=True)
                st.download_button(
                    "üì• Pobierz CSV (Wynik)",
                    df_t2.to_csv(sep=';', index=False).encode('utf-8'),
                    "wynik_z_csv.csv",
                    "text/csv"
                )

        except Exception as e:
            st.error(f"B≈ÇƒÖd odczytu pliku: {e}")



# ==========================================
# ZAK≈ÅADKA 3: ANALIZA SEMANTYCZNA (PRZENIESIONA)
# ==========================================
with tab3:
    st.header("Analiza Semantyczna (Embeddingi i Cosinusy)")
    st.markdown("Por√≥wnaj wektorowo **S≈Çowo Kluczowe** z dowolnymi innymi kolumnami (np. Tytu≈Çem, Opisem).")

    with st.expander("‚ÑπÔ∏è Jak interpretowaƒá wyniki? (≈öciƒÖga)", expanded=False):
        st.markdown("""
        **Similarity Score** to liczba od **0 do 1**, okre≈õlajƒÖca podobie≈Ñstwo znaczeniowe (semantyczne), a nie tylko obecno≈õƒá s≈Ç√≥w.
        
        * üü¢ **0.80 - 1.00**: **Bardzo mocne dopasowanie.** Fraza i tekst znaczƒÖ niemal to samo. Idealne dla tytu≈Ç√≥w SEO.
        * üü° **0.65 - 0.79**: **Dobre dopasowanie.** Temat jest zgodny, ale u≈ºyto nieco innego s≈Çownictwa. WystarczajƒÖce dla opis√≥w (meta description).
        * üü† **0.50 - 0.64**: **≈örednie dopasowanie.** Kontekst jest podobny, ale relacja jest lu≈∫na. Warto doprecyzowaƒá tre≈õƒá.
        * üî¥ **Poni≈ºej 0.50**: **S≈Çabe dopasowanie.** Algorytm uznaje, ≈ºe teksty dotyczƒÖ r√≥≈ºnych rzeczy. Ryzyko, ≈ºe Google nie powiƒÖ≈ºe frazy z tre≈õciƒÖ.
        
        üí° **Wskaz√≥wka:** Nie dƒÖ≈º do wyniku 1.0 za wszelkƒÖ cenƒô (to bywa nienaturalne). W SEO zazwyczaj celujemy w przedzia≈Ç **0.75 - 0.90**.
        """)
    
    # Sekcja pobierania szablonu
    st.subheader("1. Pobierz wz√≥r")
    st.download_button(
        label="üì• Pobierz przyk≈Çadowy CSV (Keyword + 2 kolumny)",
        data=get_semantic_template_v2().to_csv(sep=';', index=False).encode('utf-8'),
        file_name="wzor_semantyczny.csv",
        mime="text/csv"
    )
    
    st.subheader("2. Wgraj plik i wybierz kolumny")
    uploaded_sem = st.file_uploader(
        "üìÇ Wybierz plik CSV (separator ≈õrednik ';')", 
        type=['csv'], 
        key="sem_uploader_tab3" # Zmieni≈Çem key, ≈ºeby nie by≈Ço konfliktu
    )

    if uploaded_sem is not None:
        # Sprawdzamy klienta (w tym pliku jest on ju≈º zainicjalizowany wcze≈õniej)
        if client:
            try:
                # Wczytanie z separatorem ≈õrednik
                df_sem = pd.read_csv(uploaded_sem, sep=';', on_bad_lines='skip')
                
                st.success(f"‚úÖ Wczytano plik. Liczba wierszy: {len(df_sem)}")
                
                # --- DYNAMICZNY WYB√ìR KOLUMN ---
                all_columns = df_sem.columns.tolist()
                
                col1_sem, col2_sem = st.columns(2)
                
                with col1_sem:
                    # Wyb√≥r kolumny "G≈Ç√≥wnej" (S≈Çowo kluczowe)
                    keyword_col = st.selectbox(
                        "Wybierz kolumnƒô ze S≈ÅOWEM KLUCZOWYM:", 
                        options=all_columns,
                        index=0
                    )
                
                with col2_sem:
                    # Wyb√≥r kolumn do por√≥wnania
                    remaining_cols = [c for c in all_columns if c != keyword_col]
                    compare_cols = st.multiselect(
                        "Wybierz kolumny do POR√ìWNANIA (max 2):",
                        options=remaining_cols,
                        default=remaining_cols[:2] if len(remaining_cols) >= 2 else remaining_cols
                    )

                # PodglƒÖd danych
                with st.expander("üëÄ Zobacz podglƒÖd danych"):
                    st.dataframe(df_sem[[keyword_col] + compare_cols].head())

                if st.button("üöÄ Uruchom analizƒô cosinusowƒÖ"):
                    if not compare_cols:
                        st.warning("Musisz wybraƒá przynajmniej jednƒÖ kolumnƒô do por√≥wnania!")
                    else:
                        progress_text = "Obliczanie embedding√≥w..."
                        my_bar = st.progress(0, text=progress_text)
                        
                        total_rows = len(df_sem)
                        
                        # Przygotowanie s≈Çownika na wyniki
                        results_dict = {col: [] for col in compare_cols}

                        for i, row in df_sem.iterrows():
                            # 1. Embedding s≈Çowa kluczowego
                            vec_kw = get_embedding(str(row[keyword_col]), client)

                            # 2. Pƒôtla po kolumnach do por√≥wnania
                            for col_name in compare_cols:
                                vec_target = get_embedding(str(row[col_name]), client)
                                score = cosine_similarity(vec_kw, vec_target)
                                results_dict[col_name].append(round(score, 4))
                            
                            # Pasek postƒôpu
                            percent_complete = min((i + 1) / total_rows, 1.0)
                            my_bar.progress(percent_complete, text=f"Przetwarzanie wiersza {i+1} z {total_rows}")

                        # Dodanie wynik√≥w do DataFrame
                        sort_column = None
                        
                        for col_name, scores in results_dict.items():
                            new_col_name = f"score_match_{col_name}"
                            df_sem[new_col_name] = scores
                            sort_column = new_col_name

                        # Sortowanie
                        if sort_column:
                            df_sem = df_sem.sort_values(by=sort_column, ascending=True)
                        
                        my_bar.empty()
                        st.success("üéâ Analiza zako≈Ñczona!")

                        st.write("### Wyniki (posortowane wg dopasowania ostatniej kolumny):")
                        st.dataframe(df_sem.head(10))

                        st.download_button(
                            label="üì• Pobierz Raport Finalny (CSV)",
                            data=df_sem.to_csv(sep=';', index=False).encode('utf-8'),
                            file_name=f"RAPORT_FINALNY_{uploaded_sem.name}",
                            mime='text/csv',
                        )

            except Exception as e:
                st.error(f"WystƒÖpi≈Ç b≈ÇƒÖd podczas przetwarzania pliku: {e}")
                st.info("Spr√≥buj sprawdziƒá czy plik jest poprawnym CSV rozdzielonym ≈õrednikami.")
        else:
            st.error("Brak klucza API w secrets!")

# ==========================================
# ZAK≈ÅADKA 4: MATRIX KANIBALIZACJI (DODANA)
# ==========================================
with tab4:
    st.header("üï∏Ô∏è Matrix Kanibalizacji (Ka≈ºdy z Ka≈ºdym)")
    st.markdown("""
    Narzƒôdzie pobiera tre≈õƒá z listy URLi, tworzy wektory i por√≥wnuje ka≈ºdƒÖ stronƒô z ka≈ºdƒÖ innƒÖ.
    S≈Çu≈ºy do wykrywania **Duplicate Content** oraz **Kanibalizacji S≈Ç√≥w Kluczowych**.
    """)

    col_in, col_opt = st.columns([2, 1])
    
    with col_in:
        urls_matrix_input = st.text_area(
            "Wklej listƒô URLi do sprawdzenia (jeden pod drugim):", 
            height=250,
            placeholder="https://domena.pl/artykul-1\nhttps://domena.pl/artykul-2",
            key="matrix_input"
        )
    
    with col_opt:
        st.info("‚öôÔ∏è Ustawienia")
        threshold = st.slider("Poka≈º pary o podobie≈Ñstwie powy≈ºej:", 0.0, 1.0, 0.60, 0.05)
        st.caption("0.0 = Poka≈º wszystko (Pe≈Çny raport)\n0.8 = Tylko silne duplikaty")

    if st.button("üöÄ Generuj Matrix Podobie≈Ñstwa", key="btn_matrix"):
        if not client:
            st.error("Brak klucza API!")
            st.stop()
            
        # Przygotowanie listy URLi
        url_list = [u.strip() for u in urls_matrix_input.split('\n') if u.strip()]
        
        if len(url_list) < 2:
            st.warning("Podaj co najmniej 2 adresy URL, aby m√≥c je por√≥wnaƒá.")
        else:
            # Kontener na wyniki
            embeddings_list = []
            valid_urls = []
            short_names = [] # Do wykresu
            
            # UI Elementy
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # --- 1. SCRAPING I EMBEDDINGI ---
            for i, url in enumerate(url_list):
                status_text.text(f"Pobieranie i analiza ({i+1}/{len(url_list)}): {url}")
                
                # U≈ºywamy nowej funkcji extract_clean_text
                text_content = extract_clean_text(url)
                
                if text_content and len(text_content) > 100:
                    # Generowanie embeddingu (u≈ºywamy istniejƒÖcej funkcji get_embedding)
                    emb = get_embedding(text_content, client)
                    
                    embeddings_list.append(emb)
                    valid_urls.append(url)
                    
                    # Skracanie nazwy do wykresu (np. domena.pl/slug -> .../slug)
                    short_name = url.rstrip('/').split('/')[-1][:15] + "..."
                    short_names.append(short_name)
                else:
                    st.warning(f"Pominiƒôto (brak tre≈õci/b≈ÇƒÖd): {url}")
                
                progress_bar.progress((i + 1) / len(url_list))
            
            progress_bar.empty()
            status_text.success("‚úÖ Dane pobrane. Generujƒô macierz...")

            if len(embeddings_list) > 1:
                # --- 2. OBLICZENIA MACIERZOWE ---
                # U≈ºywamy sklearn dla szybko≈õci (zak≈Çadam, ≈ºe jest zainstalowany, bo by≈Ç w imports w Colabie)
                from sklearn.metrics.pairwise import cosine_similarity as sklearn_cosine
                
                matrix = sklearn_cosine(embeddings_list)
                
                # --- 3. HEATMAPA (WYKRES) ---
                st.subheader("Mapa Cieplna Podobie≈Ñstwa")
                fig, ax = plt.subplots(figsize=(10, 8))
                sns.heatmap(matrix, xticklabels=short_names, yticklabels=short_names, cmap="Greens", annot=True, fmt=".2f", ax=ax)
                st.pyplot(fig)
                
                # --- 4. TABELA WYNIK√ìW ---
                st.subheader(f"Lista Par (Podobie≈Ñstwo >= {threshold})")
                pairs = []
                all_pairs = [] # Do CSV chcemy wszystko
                
                # Przechodzimy przez g√≥rny tr√≥jkƒÖt macierzy
                for i in range(len(matrix)):
                    for j in range(i + 1, len(matrix)):
                        score = matrix[i][j]
                        
                        # Zapisz do pe≈Çnej listy
                        all_pairs.append({
                            "URL A": valid_urls[i],
                            "URL B": valid_urls[j],
                            "Podobie≈Ñstwo": round(score, 4)
                        })

                        # Zapisz do listy wy≈õwietlanej (je≈õli spe≈Çnia warunek suwaka)
                        if score >= threshold:
                            pairs.append({
                                "URL A": valid_urls[i],
                                "URL B": valid_urls[j],
                                "Podobie≈Ñstwo": round(score, 4)
                            })
                
                # Wy≈õwietlanie w aplikacji (tylko przefiltrowane)
                if pairs:
                    df_display = pd.DataFrame(pairs).sort_values(by="Podobie≈Ñstwo", ascending=False)
                    st.dataframe(df_display, use_container_width=True)
                else:
                    st.info(f"Brak par o podobie≈Ñstwie powy≈ºej {threshold}. Zmie≈Ñ ustawienie suwaka.")

                # --- 5. POBIERANIE PE≈ÅNEGO RAPORTU ---
                if all_pairs:
                    df_full = pd.DataFrame(all_pairs).sort_values(by="Podobie≈Ñstwo", ascending=False)
                    csv_data = df_full.to_csv(sep=';', index=False).encode('utf-8')
                    
                    st.download_button(
                        label="üì• Pobierz Pe≈Çny Raport Matrix (CSV)",
                        data=csv_data,
                        file_name="matrix_kanibalizacji_full.csv",
                        mime="text/csv"
                    )
            else:
                st.error("Nie uda≈Ço siƒô pobraƒá wystarczajƒÖcej liczby danych do por√≥wnania.")
