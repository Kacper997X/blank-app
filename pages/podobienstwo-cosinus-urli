import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from openai import OpenAI
import json
import bcrypt
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity
import os

# --- 1. TO MUSI BYƒÜ PIERWSZE ---
st.set_page_config(
    page_title="üïµÔ∏è SEO Matrix Analyzer", 
    page_icon="üïµÔ∏è",
    layout="wide"
)

# --- 2. AUTOMATYCZNE TWORZENIE HAS≈ÅA (NAPRAWA B≈ÅƒòDU) ---
USER_DATA_PATH = 'users.json'

def ensure_users_file_exists():
    """Tworzy plik users.json, je≈õli go brakuje."""
    if not os.path.exists(USER_DATA_PATH):
        # Generujemy has≈Ço: admin
        hashed = bcrypt.hashpw("admin".encode('utf-8'), bcrypt.gensalt()).decode('utf-8')
        data = {"users": {"admin": hashed}}
        try:
            with open(USER_DATA_PATH, 'w') as f:
                json.dump(data, f)
            return True
        except Exception as e:
            st.error(f"B≈ÇƒÖd zapisu pliku: {e}")
            return False
    return False

# --- 3. SYSTEM LOGOWANIA ---
def check_password(hashed_password, user_password):
    return bcrypt.checkpw(user_password.encode('utf-8'), hashed_password.encode('utf-8'))

def login_screen():
    created_new = ensure_users_file_exists()
    
    # Pr√≥ba odczytu pliku
    try:
        with open(USER_DATA_PATH, 'r') as file:
            users = json.load(file)['users']
    except Exception:
        # Je≈õli plik jest uszkodzony, nadpisujemy go
        ensure_users_file_exists()
        with open(USER_DATA_PATH, 'r') as file:
            users = json.load(file)['users']

    # Je≈õli utworzono nowy plik, poka≈º komunikat
    if created_new:
        st.warning("‚ö†Ô∏è Stworzono nowy plik hase≈Ç.")
        st.info("Login: **admin** | Has≈Ço: **admin**")

    st.title("üîê Zaloguj siƒô")
    
    col1, col2 = st.columns([1, 2])
    with col1:
        username = st.text_input("U≈ºytkownik")
        password = st.text_input("Has≈Ço", type="password")
        
        if st.button("Zaloguj", type="primary"):
            if username in users and check_password(users[username], password):
                st.session_state['logged_in'] = True
                st.session_state['username'] = username
                st.rerun()
            else:
                st.error("B≈Çƒôdne dane.")

# Sprawdzenie stanu logowania
if 'logged_in' not in st.session_state:
    st.session_state['logged_in'] = False

if not st.session_state['logged_in']:
    login_screen()
    st.stop()  # ZATRZYMUJE KOD TU, JE≈öLI NIE ZALOGOWANO

# ==========================================
# 3. INTERFEJS U≈ªYTKOWNIKA (SIDEBAR)
# ==========================================

st.sidebar.title(f"üë§ {st.session_state.get('username', 'User')}")
if st.sidebar.button("Wyloguj"):
    logout()

st.sidebar.markdown("---")
st.sidebar.header("‚öôÔ∏è Konfiguracja")

# Klucz API
api_key = st.sidebar.text_input("OpenAI API Key", type="password", help="Podaj klucz, je≈õli nie jest ustawiony w secrets.")
if not api_key:
    # Pr√≥ba pobrania ze zmiennych ≈õrodowiskowych lub secrets
    try:
        api_key = st.secrets["OPENAI_API_KEY"]
    except:
        api_key = os.environ.get("OPENAI_API_KEY")

# Suwak progu (THRESHOLD)
threshold = st.sidebar.slider(
    "Pr√≥g podobie≈Ñstwa", 
    min_value=0.0, 
    max_value=1.0, 
    value=0.5, 
    step=0.05,
    help="Pary powy≈ºej tej warto≈õci zostanƒÖ oznaczone jako duplikaty."
)

st.sidebar.markdown("---")
st.sidebar.info("Aplikacja do analizy kanibalizacji tre≈õci (Semantic SEO).")

# ==========================================
# 4. FUNKCJE ANALIZY (BACKEND)
# ==========================================

# Dekorator cache - zapobiega ponownemu scrapowaniu tych samych URLi przy zmianie suwaka
@st.cache_data(show_spinner=False)
def extract_clean_text(url):
    """Scraper wycinajƒÖcy menu i stopki."""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200: return None

        soup = BeautifulSoup(response.content, 'html.parser')

        # Usuwamy techniczne
        for element in soup(["script", "style", "nav", "footer", "header", "aside", "form", "iframe", "noscript", "svg"]):
            element.decompose()

        text = soup.get_text(separator='\n')
        lines = text.split('\n')
        clean_lines = []

        for line in lines:
            line = line.strip()
            if len(line) < 30: continue
            garbage_words = ["wszelkie prawa", "copyright", "polityka prywatno≈õci", "pliki cookies", "zobacz wiƒôcej", "czytaj dalej", "sprawd≈∫ ofertƒô", "menu stopki", "mapa witryny"]
            if any(word in line.lower() for word in garbage_words): continue
            clean_lines.append(line)

        final_text = ' '.join(clean_lines)
        # Fallback je≈õli czyszczenie usunƒô≈Ço za du≈ºo
        if len(final_text) < 100: 
            raw_text = soup.get_text(separator=' ')
            return raw_text[:5000] 
            
        return final_text[:20000]
    except Exception as e:
        logger.error(f"B≈ÇƒÖd scrapowania {url}: {e}")
        return None

def get_embedding(text, client, model="text-embedding-3-large"):
    text = text.replace("\n", " ")
    return client.embeddings.create(input=[text], model=model).data[0].embedding

def perform_analysis(url_list_raw, api_key_val):
    """G≈Ç√≥wna funkcja wykonujƒÖca analizƒô."""
    client = OpenAI(api_key=api_key_val)
    
    urls = [line.strip() for line in url_list_raw.split('\n') if line.strip()]
    
    if not urls:
        st.error("Lista URLi jest pusta!")
        return None

    progress_bar = st.progress(0)
    status_text = st.empty()
    
    data_list = []
    embeddings = []
    
    total = len(urls)
    
    # ETAP 1: Scraping i Wektoryzacja
    for i, url in enumerate(urls):
        status_text.text(f"Analizujƒô ({i+1}/{total}): {url}")
        text = extract_clean_text(url)
        
        if text and len(text) > 100:
            try:
                emb = get_embedding(text, client)
                embeddings.append(emb)
                
                short_name = url.rstrip('/').split('/')[-1]
                if not short_name: short_name = url # Fallback dla domeny g≈Ç√≥wnej
                short_name = short_name[:25] + "..." if len(short_name) > 25 else short_name
                
                data_list.append({
                    'url': url,
                    'short_name': short_name,
                    'text_preview': text[:200]
                })
            except Exception as e:
                st.warning(f"B≈ÇƒÖd API dla {url}: {e}")
        else:
            st.warning(f"Pominiƒôto (brak tre≈õci): {url}")
        
        progress_bar.progress((i + 1) / total)
        time.sleep(0.1) 

    status_text.text("Obliczam podobie≈Ñstwo...")
    
    if len(embeddings) < 2:
        st.error("Za ma≈Ço danych do por√≥wnania (wymagane min. 2 poprawne URLe).")
        return None

    # ETAP 2: Macierz
    matrix = cosine_similarity(embeddings)
    
    return {
        "matrix": matrix,
        "data": data_list
    }

# ==========================================
# 5. G≈Å√ìWNY WIDOK APLIKACJI
# ==========================================

st.title("üïµÔ∏è Masowa Analiza Podobie≈Ñstwa (Matrix)")
st.markdown("Wklej listƒô adres√≥w URL, aby znale≈∫ƒá duplikaty i kanibalizacjƒô tre≈õci.")

# Zmiana: Usuniƒôto domy≈õlne URLe. Teraz pole jest puste i czeka na wklejenie danych przez u≈ºytkownika.
url_input = st.text_area(
    "Lista URLi (jeden pod drugim):", 
    height=200, 
    placeholder="https://twojastrona.pl/\nhttps://twojastrona.pl/oferta\nhttps://twojastrona.pl/blog/artykul-1",
    help="Wklej ka≈ºdy adres URL w nowej linii."
)

col_act1, col_act2 = st.columns([1, 4])
with col_act1:
    run_btn = st.button("üöÄ Uruchom Analizƒô", type="primary")

# Obs≈Çuga przycisku START
if run_btn:
    # Walidacja: Sprawdzamy, czy pole nie jest puste
    if not url_input.strip():
        st.error("‚ö†Ô∏è Pole z adresami URL jest puste! Wklej listƒô adres√≥w, aby kontynuowaƒá.")
    elif not api_key:
        st.error("‚ùå Brak klucza API OpenAI! Ustaw go w pasku bocznym.")
    else:
        with st.spinner("Pobieranie stron i generowanie wektor√≥w..."):
            result = perform_analysis(url_input, api_key)
            
            if result:
                st.session_state['analysis_done'] = True
                st.session_state['matrix'] = result['matrix']
                st.session_state['valid_urls_data'] = result['data']

# ==========================================
# 6. WYNIKI I RAPORTOWANIE
# ==========================================

if st.session_state.get('analysis_done'):
    
    matrix = st.session_state['matrix']
    data_items = st.session_state['valid_urls_data']
    labels = [d['short_name'] for d in data_items]
    full_urls = [d['url'] for d in data_items]

    st.markdown("---")
    
    # --- A. HEATMAPA ---
    st.subheader("1. Mapa Ciep≈Ça (Heatmap)")
    
    fig, ax = plt.subplots(figsize=(12, 10))
    sns.heatmap(matrix, xticklabels=labels, yticklabels=labels, cmap="Greens", annot=True, fmt=".2f", ax=ax)
    plt.xticks(rotation=45, ha="right")
    plt.title("Macierz Podobie≈Ñstwa Semantycznego")
    st.pyplot(fig)

    # --- B. PRZETWARZANIE DANYCH POD SUWAK ---
    st.subheader(f"2. Raport Par (Pr√≥g >= {threshold})")
    
    pairs = []
    all_pairs = []

    for i in range(len(matrix)):
        for j in range(i + 1, len(matrix)):
            score = matrix[i][j]
            
            # Pe≈Çna lista
            all_pairs.append([full_urls[i], full_urls[j], score])
            
            # Lista filtrowana
            if score >= threshold:
                pairs.append([full_urls[i], full_urls[j], score])

    # --- WIDOK TABELI ---
    if pairs:
        df_pairs = pd.DataFrame(pairs, columns=["URL A", "URL B", "Score"])
        df_pairs = df_pairs.sort_values(by="Score", ascending=False)
        
        # Kolorowanie wynik√≥w w tabeli
        st.dataframe(
            df_pairs.style.background_gradient(subset=['Score'], cmap="Greens", vmin=0, vmax=1),
            use_container_width=True
        )
        
        # Pobieranie CSV Filtrowanego
        csv_filtered = df_pairs.to_csv(index=False, sep=";").encode('utf-8-sig')
        st.download_button(
            label=f"üíæ Pobierz pary > {threshold} (CSV)",
            data=csv_filtered,
            file_name="znalezione_duplikaty.csv",
            mime="text/csv"
        )
    else:
        st.info(f"‚úÖ Brak par o podobie≈Ñstwie powy≈ºej {threshold}. Twoje tre≈õci sƒÖ unikalne!")

    # --- POBIERANIE PE≈ÅNEGO RAPORTU ---
    st.markdown("---")
    st.subheader("3. Pe≈Çny zrzut danych")
    
    df_all = pd.DataFrame(all_pairs, columns=["URL A", "URL B", "Score"])
    csv_all = df_all.to_csv(index=False, sep=";").encode('utf-8-sig')
    
    st.download_button(
        label="üíæ Pobierz PE≈ÅNƒÑ macierz (wszystkie pary)",
        data=csv_all,
        file_name="pelny_raport_matrix.csv",
        mime="text/csv"
    )
